{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "618b6920-b4e0-4689-a109-bc423a3ac1a2",
   "metadata": {},
   "source": [
    "# **The Linear Algebra Behind Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f56aa4-19c2-4988-bf20-928431fcd7df",
   "metadata": {},
   "source": [
    "``` python\n",
    "x = [1, 1.5, 6, 2, 3]\n",
    "y = [4, 7, 12, 8, 7]\n",
    "\n",
    "X = np.asarray([np.ones(5), x]).T\n",
    "print(X)\n",
    ">>> [[1. 1. ]\n",
    ">>> [1. 1.5]\n",
    ">>> [1. 6. ]\n",
    ">>> [1. 2. ]\n",
    ">>> [1. 3. ]]\n",
    "\n",
    "from numpy.linalg import inv\n",
    "beta_0, beta_1 = inv(X.T @ X) @ X.T @ y\n",
    "print(beta_0, beta_1)\n",
    ">>> (4.028481012658229, 1.3227848101265818)\n",
    "\n",
    "x_lin_space = np.linspace(0, 7, 100)\n",
    "y_hat = beta_0 + beta_1 * x_lin_space\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3ea16-f80a-4c30-8c99-daa19fac59ca",
   "metadata": {},
   "source": [
    "*Независимите променливи x са стойностите, които използваме за предсказване. Например, ако искаме да предскажем цената на къща според квадратурата ѝ, x е квадратурата. Зависимата променлива y е стойността, която искаме да предскажем, в случая цената на къщата. Линейната регресия има за цел да намери най-добрата права линия, която да описва връзката между x и y. Линейната регресия се чрез формулата:*\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "*beta_0 е свободният член - точката, в която линията пресича оста y. За да можем да използваме формулата на обикновените най-малки квадрати, добавяме колона с единици в матрицата X. Eквивалентно e на това, че beta_0 ще се умножава по 1, т.е. ще бъде включено като свободен член. Използваме формулата за обикновени най-малки квадрати:*\n",
    "\n",
    "$$\n",
    "\\beta = \n",
    "\\begin{bmatrix} \n",
    "\\beta_0 \\\\ \n",
    "\\beta_1 \n",
    "\\end{bmatrix} \n",
    "= (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "*Целта на формулата е да се намерят стойностите на beta_0 и beta_1, които минимизират сумата на квадратите на разликите между предсказаните и реалните y:*\n",
    "\n",
    "$$\n",
    "\\text{minimize } \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "*Тук: X^T.X е матрица на взаимната зависимост между колоните на X. (X^T.X)^{-1} е обратната матрица, която позволява решаване на системата. X^T.y комбинира наблюдаваните y със съответните x. Като умножим, получаваме най-добрите коефициенти beta_0 и beta_1, които минимизират грешката. Резултатът за конкретния пример е:*\n",
    "\n",
    "$$\n",
    "\\beta_0 \\approx 4.028, \\quad \\beta_1 \\approx 1.323\n",
    "$$\n",
    "\n",
    "*Следователно, най-добрата права е:*\n",
    "\n",
    "$$\n",
    "\\hat{y} = 4.028 + 1.323\\,x\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
