{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c07f3ec-31db-4418-ac75-0eb66d8ca5cf",
   "metadata": {},
   "source": [
    "## **Зад 1. Четирите закона наведнъж**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca593404-2aa2-4496-a067-ec647c373c2b",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\frac{\\sin(2x^5 + 3x)}{e^{7x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea26f4b-f95e-4915-afff-11a0a2775593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) = [0.14676048 0.15197726 0.15313118 0.1511604  0.14684032]\n",
      "f'(x) ≈ [ 0.27182196  0.1659735  -0.02128136 -0.1638934  -0.2697288 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0.1, 2, 100)\n",
    "\n",
    "f = np.sin(2*x**5 + 3*x) / np.exp(7*x)\n",
    "df_dx = np.gradient(f, x)\n",
    "\n",
    "print(\"f(x) =\", f[:5])\n",
    "print(\"f'(x) ≈\", df_dx[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f42a8-7e1c-4374-9994-9d0a8a14a528",
   "metadata": {},
   "source": [
    "## **Зад 2. XOR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15731e04-b579-4dd1-8587-28a07eb26ca3",
   "metadata": {},
   "source": [
    "### tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c733c3-16d2-4369-a4a0-25248e5609ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Input: [0 0] => Predicted Output: [0], Actual Output: [0]\n",
      "Input: [0 1] => Predicted Output: [1], Actual Output: [1]\n",
      "Input: [1 0] => Predicted Output: [1], Actual Output: [1]\n",
      "Input: [1 1] => Predicted Output: [0], Actual Output: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return 1 - x**2 \n",
    "\n",
    "np.random.seed(42)\n",
    "input_dim = 2\n",
    "hidden_dim = 4 \n",
    "output_dim = 1\n",
    "\n",
    "W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = tanh(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = tanh(z2)\n",
    "\n",
    "    # Mean Squared Error има същият диапазон като tanh() [-1,1]\n",
    "    loss = np.mean(((y * 2 - 1) - a2)**2)\n",
    "\n",
    "    # Backpropagation\n",
    "    dz2 = (a2 - (y * 2 - 1)) * tanh_derivative(a2)\n",
    "    dW2 = np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * tanh_derivative(a1)\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "\n",
    "a1 = tanh(np.dot(X, W1) + b1)\n",
    "a2 = tanh(np.dot(a1, W2) + b2)\n",
    "predictions = ((a2 + 1) / 2).round().astype(int)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]} => Predicted Output: {predictions[i]}, Actual Output: {y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e8cd6-5e29-44ab-a330-0e799bf33d9d",
   "metadata": {},
   "source": [
    "### sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34422f87-fc91-46b9-8f9d-a9649b7609c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n",
      "Input: [0 0] => Predicted Output: [0], Actual Output: [0]\n",
      "Input: [0 1] => Predicted Output: [1], Actual Output: [1]\n",
      "Input: [1 0] => Predicted Output: [1], Actual Output: [1]\n",
      "Input: [1 1] => Predicted Output: [1], Actual Output: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "np.random.seed(42)\n",
    "input_dim = 2\n",
    "hidden_dim = 4 \n",
    "output_dim = 1\n",
    "\n",
    "W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Binary Cross-Entropy Loss има същият диапазон като sigmoid() [0,1]\n",
    "    loss = -np.mean(y * np.log(a2 + 1e-8) + (1 - y) * np.log(1 - a2 + 1e-8))\n",
    "\n",
    "    # Backpropagation\n",
    "    dz2 = a2 - y\n",
    "    dW2 = np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * sigmoid_derivative(a1)\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "\n",
    "a1 = sigmoid(np.dot(X, W1) + b1)\n",
    "a2 = sigmoid(np.dot(a1, W2) + b2)\n",
    "predictions = np.round(a2).astype(int)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]} => Predicted Output: {predictions[i]}, Actual Output: {y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01da6eec-e24b-4e60-a725-ee10a31bb222",
   "metadata": {},
   "source": [
    "### relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eda39fc-7b61-4001-b811-84ae4b063451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 50.00%\n",
      "Input: [0 0] => Predicted Output: [0], Actual Output: [0]\n",
      "Input: [0 1] => Predicted Output: [0], Actual Output: [1]\n",
      "Input: [1 0] => Predicted Output: [0], Actual Output: [1]\n",
      "Input: [1 1] => Predicted Output: [0], Actual Output: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "np.random.seed(42)\n",
    "input_dim = 2\n",
    "hidden_dim = 4 \n",
    "output_dim = 1\n",
    "\n",
    "W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "lr = 0.01 \n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = relu(z2)\n",
    "\n",
    "    # Mean Squared Error loss\n",
    "    loss = np.mean((y - a2)**2)\n",
    "\n",
    "    # Backpropagation\n",
    "    dz2 = (a2 - y) * relu_derivative(a2)\n",
    "    dW2 = np.dot(a1.T, dz2)\n",
    "    db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    da1 = np.dot(dz2, W2.T)\n",
    "    dz1 = da1 * relu_derivative(a1)\n",
    "    dW1 = np.dot(X.T, dz1)\n",
    "    db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "\n",
    "a1 = relu(np.dot(X, W1) + b1)\n",
    "a2 = relu(np.dot(a1, W2) + b2)\n",
    "predictions = np.round(a2).astype(int)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]} => Predicted Output: {predictions[i]}, Actual Output: {y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc1e39-db8d-4227-b4f5-a11132926d75",
   "metadata": {},
   "source": [
    "## **Зад 3. Точки в кръг**\n",
    "- Комбинацията от `ReLU` в скритите слоеве и `sigmoid` в изходния слой работи най-добре за бинарна класификация, защото всяка функция изпълнява оптимална роля: `ReLU` в скритите слоеве позволява бързо и стабилно обучение, като представяния на данните чрез връщане на `0` за отрицателни стойности и линейна трансформация за положителни, докато `Sigmoid` в изходния слой нормализира резултата между `0` и `1`. Ако се използва ReLU и в изхода, стойностите могат да надхвърлят `1`, което разрушава обучението."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2167666e-3580-40f7-8051-9ba40381599a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9340\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.uniform(-1.5, 1.5, (500, 2))\n",
    "Y = (X[:, 0]**2 + X[:, 1]**2 <= 1).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "# relu за скрит слой\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# sigmoid за изход\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    loss = -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    dA2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T, dA2) / X.shape[0]\n",
    "    db2 = np.sum(dA2, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    dA1 = np.dot(dA2, W2.T) * relu_derivative(A1)\n",
    "    dW1 = np.dot(X.T, dA1) / X.shape[0]\n",
    "    db1 = np.sum(dA1, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "predictions = (A2 > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4bfd5-2199-487f-8023-a3b69cc59264",
   "metadata": {},
   "source": [
    "## **Зад 4. Точки в елипса**\n",
    "$$\n",
    "\\text{ELU}(x) =\n",
    "\\begin{cases} \n",
    "x & x > 0 \\\\\n",
    "\\alpha (e^x - 1) & x \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ELU}'(x) =\n",
    "\\begin{cases} \n",
    "1 & x > 0 \\\\\n",
    "\\text{ELU}(x) + \\alpha & x \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- `ELU` (Exponential Linear Unit) може да се разглежда като надграждане на `ReLU` по два ключови начина:\n",
    "    - Обработка на отрицателните стойности: `ReLU` задава всички отрицателни входове на `0`. Това води до неврони, които никога **не** се активират и за тях градиентът е `0`. `ELU` вместо това връща плавна експоненциална функция за отрицателните входове. Така градиентът **не** изчезва и невронът продължава да се учи дори при отрицателни стойности.\n",
    "    - Центриране около нула: Изходът на `ReLU` е `[0, inf)`, което води до положителни средни стойности и може да забави обучението. `ELU` има отрицателни стойности за отрицателни входове, така че средната стойност на активирацията е по-близо до `0`. Това подобрява стабилността на градиентите и ускорява конвергенцията."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09a4ef9-3074-4c4e-97a8-3e4dd28e98b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9800\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.uniform(-2, 2, (100, 2))\n",
    "Y = ((X[:,0]**2 / 1.5**2 + X[:,1]**2 / 1**2) <= 1).astype(np.float32).reshape(-1,1)\n",
    "\n",
    "# ELU за скрития слой\n",
    "alpha = 1.0\n",
    "def elu(x):\n",
    "    return np.where(x > 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def elu_derivative(x):\n",
    "    return np.where(x > 0, 1, elu(x) + alpha)\n",
    "\n",
    "# sigmoid за изхода\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = elu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    loss = -np.mean(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    \n",
    "    dA2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T, dA2) / X.shape[0]\n",
    "    db2 = np.sum(dA2, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    dA1 = np.dot(dA2, W2.T) * elu_derivative(A1)\n",
    "    dW1 = np.dot(X.T, dA1) / X.shape[0]\n",
    "    db1 = np.sum(dA1, axis=0, keepdims=True) / X.shape[0]\n",
    "\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "predictions = (A2 > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ab0a19-f201-45ff-ba1f-596bfa276056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
