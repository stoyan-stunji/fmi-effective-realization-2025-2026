{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c07f3ec-31db-4418-ac75-0eb66d8ca5cf",
   "metadata": {},
   "source": [
    "## **Зад 1. Четирите закона наведнъж**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca593404-2aa2-4496-a067-ec647c373c2b",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\frac{\\sin(2x^5 + 3x)}{e^{7x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea26f4b-f95e-4915-afff-11a0a2775593",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m x = np.linspace(\u001b[32m0.1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m100\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# write code here\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mf(x) =\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mf\u001b[49m[:\u001b[32m5\u001b[39m])\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m(x) ≈\u001b[39m\u001b[33m\"\u001b[39m, df_dx[:\u001b[32m5\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0.1, 2, 100)\n",
    "\n",
    "# write code here\n",
    "\n",
    "print(\"f(x) =\", f[:5])\n",
    "print(\"f'(x) ≈\", df_dx[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f42a8-7e1c-4374-9994-9d0a8a14a528",
   "metadata": {},
   "source": [
    "## **Зад 2. XOR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15731e04-b579-4dd1-8587-28a07eb26ca3",
   "metadata": {},
   "source": [
    "### tanh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c733c3-16d2-4369-a4a0-25248e5609ed",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 9 (2886062802.py, line 12)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef activation_derivative(x):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "def activation(x):\n",
    "    # write code here\n",
    "\n",
    "def activation_derivative(x):\n",
    "    # write code here\n",
    "\n",
    "np.random.seed(42)\n",
    "input_dim = 2\n",
    "hidden_dim = 4 \n",
    "output_dim = 1\n",
    "\n",
    "W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "b2 = np.zeros((1, output_dim))\n",
    "\n",
    "lr = 0.1\n",
    "epochs = 20000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    # write code here\n",
    "\n",
    "    # Backpropagation\n",
    "    # write code here\n",
    "\n",
    "a1 = activation(np.dot(X, W1) + b1)\n",
    "a2 = activation(np.dot(a1, W2) + b2)\n",
    "predictions = ((a2 + 1) / 2).round().astype(int)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input: {X[i]} => Predicted Output: {predictions[i]}, Actual Output: {y[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc1e39-db8d-4227-b4f5-a11132926d75",
   "metadata": {},
   "source": [
    "## **Зад 3. Точки в кръг**\n",
    "- Комбинацията от `ReLU` в скритите слоеве и `sigmoid` в изходния слой работи най-добре за бинарна класификация, защото всяка функция изпълнява оптимална роля: `ReLU` в скритите слоеве позволява бързо и стабилно обучение, като представяния на данните чрез връщане на `0` за отрицателни стойности и линейна трансформация за положителни, докато `Sigmoid` в изходния слой нормализира резултата между `0` и `1`. Ако се използва ReLU и в изхода, стойностите могат да надхвърлят `1`, което разрушава обучението."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2167666e-3580-40f7-8051-9ba40381599a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6860\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.uniform(-1.5, 1.5, (500, 2))\n",
    "Y = (X[:, 0]**2 + X[:, 1]**2 <= 1).astype(np.float32).reshape(-1, 1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "        \n",
    "    dA2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T, dA2) / X.shape[0]\n",
    "    db2 = np.sum(dA2, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    dA1 = np.dot(dA2, W2.T) * sigmoid_derivative(A1)\n",
    "    dW1 = np.dot(X.T, dA1) / X.shape[0]\n",
    "    db1 = np.sum(dA1, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "predictions = (A2 > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4bfd5-2199-487f-8023-a3b69cc59264",
   "metadata": {},
   "source": [
    "## **Зад 4. Точки в елипса**\n",
    "$$\n",
    "\\text{ELU}(x) =\n",
    "\\begin{cases} \n",
    "x & x > 0 \\\\\n",
    "\\alpha (e^x - 1) & x \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ELU}'(x) =\n",
    "\\begin{cases} \n",
    "1 & x > 0 \\\\\n",
    "\\text{ELU}(x) + \\alpha & x \\le 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- `ELU` (Exponential Linear Unit) може да се разглежда като надграждане на `ReLU` по два ключови начина:\n",
    "    - Обработка на отрицателните стойности: `ReLU` задава всички отрицателни входове на `0`. Това води до неврони, които никога **не** се активират и за тях градиентът е `0`. `ELU` вместо това връща плавна експоненциална функция за отрицателните входове. Така градиентът **не** изчезва и невронът продължава да се учи дори при отрицателни стойности.\n",
    "    - Центриране около нула: Изходът на `ReLU` е `[0, inf)`, което води до положителни средни стойности и може да забави обучението. `ELU` има отрицателни стойности за отрицателни входове, така че средната стойност на активирацията е по-близо до `0`. Това подобрява стабилността на градиентите и ускорява конвергенцията."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09a4ef9-3074-4c4e-97a8-3e4dd28e98b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 8 (2919010987.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdef func_derivative(x):\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.uniform(-2, 2, (100, 2))\n",
    "Y = ((X[:,0]**2 / 1.5**2 + X[:,1]**2 / 1**2) <= 1).astype(np.float32).reshape(-1,1)\n",
    "\n",
    "# ? за скрития слой\n",
    "def func(x):\n",
    "    # write code here\n",
    "\n",
    "def func_derivative(x):\n",
    "    # write code here\n",
    "\n",
    "# sigmoid за изхода\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "input_size = 2\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    A1 = func(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "        \n",
    "    dA2 = A2 - Y\n",
    "    dW2 = np.dot(A1.T, dA2) / X.shape[0]\n",
    "    db2 = np.sum(dA2, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    dA1 = np.dot(dA2, W2.T) * func_derivative(A1)\n",
    "    dW1 = np.dot(X.T, dA1) / X.shape[0]\n",
    "    db1 = np.sum(dA1, axis=0, keepdims=True) / X.shape[0]\n",
    "    \n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "\n",
    "predictions = (A2 > 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == Y)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc32076-42b9-439c-9462-a548906529b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
