{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab89774-a56c-47a2-aeec-43c0689c7ee5",
   "metadata": {},
   "source": [
    "# **[Principal Component Analysis (PCA) from Scratch](https://bagheri365.github.io/blog/Principal-Component-Analysis-from-Scratch/)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca72718c-cdee-4e88-bafa-5a84d6f3deeb",
   "metadata": {},
   "source": [
    "## Background \n",
    "Principal Component Analysis (PCA) is a simple dimensionality reduction technique that can capture linear correlations between the features. For a given (standardized) data, PCA can be calculated by eigenvalue decomposition of covariance (or correlation) matrix of the data, or Singular Value Decomposition (SVD) of the data matrix. The data standardization includes mean removal and variance normalization.\n",
    "\n",
    "## Data Example \n",
    "In this project, we use iris dataset. The data set consists of 50 samples from each of three species of Iris. The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
    "\n",
    "``` python\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print('Number of samples:', n_samples)\n",
    "print('Number of features:', n_features)\n",
    "\n",
    "# Венчелистчето (petal) е частта на цветето, която обикновено е ярко оцветена и привлича насекоми за опрашване.\n",
    "# Чашелистчето (sepal) е външният листовиден защитен слой на цветето, който покрива и защитава пъпката преди да се разтвори.\n",
    "# т.е. Дължината и ширината на чашелистчетата и венчелистчетата са нашите атрибути - 2х2=4  \n",
    "```\n",
    "``` \n",
    "Number of samples: 150\n",
    "Number of features: 4\n",
    "```\n",
    "To get a feeling for how features (independent variables) are related, let us visualize them via histograms and scatter plots.\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=n_features, ncols=n_features, figsize= (8, 8))\n",
    "fig.tight_layout()\n",
    "\n",
    "names = iris.feature_names\n",
    "\n",
    "for i, j in zip(*np.triu_indices_from(ax, k=1)):\n",
    "    ax[j, i].scatter(X[:, j], X[:, i], c = y)\n",
    "    ax[j, i].set_xlabel(names[j])\n",
    "    ax[j, i].set_ylabel(names[i])\n",
    "    ax[i, j].set_axis_off()\n",
    "\n",
    "for i in range(n_features):\n",
    "    ax[i, i].hist(X[:, i], color = 'lightblue')\n",
    "    ax[i, i].set_ylabel('Count')\n",
    "    ax[i, i].set_xlabel(names[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a5b5f-0891-426c-848e-206f257ab95c",
   "metadata": {},
   "source": [
    "## Background \n",
    "Principal Component Analysis (PCA) is a simple dimensionality reduction technique that can capture linear correlations between the features. For a given (standardized) data, PCA can be calculated by eigenvalue decomposition of covariance (or correlation) matrix of the data, or Singular Value Decomposition (SVD) of the data matrix. The data standardization includes mean removal and variance normalization.\n",
    "\n",
    "## Data Example \n",
    "In this project, we use iris dataset. The data set consists of 50 samples from each of three species of Iris. The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
    "\n",
    "``` python\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris['target']\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "print('Number of samples:', n_samples)\n",
    "print('Number of features:', n_features)\n",
    "\n",
    "# Венчелистчето (petal) е частта на цветето, която обикновено е ярко оцветена и привлича насекоми за опрашване.\n",
    "# Чашелистчето (sepal) е външният листовиден защитен слой на цветето, който покрива и защитава пъпката преди да се разтвори.\n",
    "# т.е. Дължината и ширината на чашелистчетата и венчелистчетата са нашите атрибути - 2х2=4  \n",
    "```\n",
    "``` \n",
    "Number of samples: 150\n",
    "Number of features: 4\n",
    "```\n",
    "To get a feeling for how features (independent variables) are related, let us visualize them via histograms and scatter plots.\n",
    "``` python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots(nrows=n_features, ncols=n_features, figsize= (8, 8))\n",
    "fig.tight_layout()\n",
    "\n",
    "names = iris.feature_names\n",
    "\n",
    "for i, j in zip(*np.triu_indices_from(ax, k=1)):\n",
    "    ax[j, i].scatter(X[:, j], X[:, i], c = y)\n",
    "    ax[j, i].set_xlabel(names[j])\n",
    "    ax[j, i].set_ylabel(names[i])\n",
    "    ax[i, j].set_axis_off()\n",
    "\n",
    "for i in range(n_features):\n",
    "    ax[i, i].hist(X[:, i], color = 'lightblue')\n",
    "    ax[i, i].set_ylabel('Count')\n",
    "    ax[i, i].set_xlabel(names[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a4a96e-f8b2-4bf2-bf76-cd82fbad013f",
   "metadata": {},
   "source": [
    "## PCA with the covariance method \n",
    "The following step-by-step guide explains the general framework for computing PCA using the covariance method.\n",
    "\n",
    "## Step 1: Standardize the data \n",
    "We can standardize features by removing the mean and scaling to unit variance.\n",
    "\n",
    "``` python\n",
    "def mean(x): # np.mean(X, axis = 0)  \n",
    "    return sum(x)/len(x)  \n",
    "\n",
    "def std(x): # np.std(X, axis = 0)\n",
    "    return (sum((i - mean(x))**2 for i in x)/len(x))**0.5\n",
    "\n",
    "def Standardize_data(X):\n",
    "    return (X - mean(X))/std(X)\n",
    "\n",
    "X_std = Standardize_data(X)\n",
    "```\n",
    "\n",
    "## Step 2: Find the covariance matrix \n",
    "The covariance matrix of standardized data can be calculated as follows.\n",
    "\n",
    "``` python\n",
    "def covariance(x): \n",
    "    return (x.T @ x)/(x.shape[0]-1)\n",
    "\n",
    "cov_mat = covariance(X_std) # np.cov(X_std.T)\n",
    "```\n",
    "\n",
    "## Step 3: Find the eigenvectors and eigenvalues of the covariance matrix\n",
    "``` python\n",
    "from numpy.linalg import eig\n",
    "\n",
    "# Eigendecomposition of covariance matrix\n",
    "eig_vals, eig_vecs = eig(cov_mat) \n",
    "\n",
    "# Adjusting the eigenvectors (loadings) that are largest in absolute value to be positive\n",
    "max_abs_idx = np.argmax(np.abs(eig_vecs), axis=0)\n",
    "signs = np.sign(eig_vecs[max_abs_idx, range(eig_vecs.shape[0])])\n",
    "eig_vecs = eig_vecs*signs[np.newaxis,:]\n",
    "eig_vecs = eig_vecs.T\n",
    "\n",
    "print('Eigenvalues \\n', eig_vals)\n",
    "print('Eigenvectors \\n', eig_vecs)\n",
    "```\n",
    "```\n",
    "Eigenvalues \n",
    " [2.93808505 0.9201649  0.14774182 0.02085386]\n",
    "Eigenvectors \n",
    " [[ 0.52106591 -0.26934744  0.5804131   0.56485654]\n",
    " [ 0.37741762  0.92329566  0.02449161  0.06694199]\n",
    " [ 0.71956635 -0.24438178 -0.14212637 -0.63427274]\n",
    " [-0.26128628  0.12350962  0.80144925 -0.52359713]]\n",
    "```\n",
    "\n",
    "## Step 4: Rearrange the eigenvectors and eigenvalues \n",
    "Here, we sort eigenvalues in descending order.\n",
    "\n",
    "``` python\n",
    "# We first make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[i,:]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Then, we sort the tuples from the highest to the lowest based on eigenvalues magnitude\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# For further usage\n",
    "eig_vals_sorted = np.array([x[0] for x in eig_pairs])\n",
    "eig_vecs_sorted = np.array([x[1] for x in eig_pairs])\n",
    "\n",
    "print(eig_pairs)\n",
    "```\n",
    "```\n",
    "[(2.9380850501999953, array([ 0.52106591, -0.26934744,  0.5804131 ,  0.56485654])), (0.9201649041624866, array([0.37741762, 0.92329566, 0.02449161, 0.06694199])), (0.147741821044948, array([ 0.71956635, -0.24438178, -0.14212637, -0.63427274])), (0.020853862176462217, array([-0.26128628,  0.12350962,  0.80144925, -0.52359713]))]\n",
    "``` \n",
    "\n",
    "## Step 5: Choose principal components \n",
    "Now, we choose the first k eigenvectors where  k is the number of dimensions of the new feature subspace (k ≤ n features).\n",
    "\n",
    "``` python\n",
    "# Select top k eigenvectors\n",
    "k = 2\n",
    "W = eig_vecs_sorted[:k, :] # Projection matrix\n",
    "\n",
    "print(W.shape)\n",
    "(2, 4)\n",
    "\n",
    "# Note that, the value of k can be set in a wiser way through explained variance. The explained variance tells us how much information (variance) can be attributed to each of the principal components.\n",
    "```\n",
    "``` python\n",
    "eig_vals_total = sum(eig_vals)\n",
    "explained_variance = [(i / eig_vals_total)*100 for i in eig_vals_sorted]\n",
    "explained_variance = np.round(explained_variance, 2)\n",
    "cum_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print('Explained variance: {}'.format(explained_variance))\n",
    "print('Cumulative explained variance: {}'.format(cum_explained_variance))\n",
    "\n",
    "plt.plot(np.arange(1,n_features+1), cum_explained_variance, '-o')\n",
    "plt.xticks(np.arange(1,n_features+1))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "plt.show()\n",
    "```\n",
    "```\n",
    "Explained variance: [72.96 22.85  3.67  0.52]\n",
    "Cumulative explained variance: [ 72.96  95.81  99.48 100.  ]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be37e1-d278-47a0-9c57-e13813df01d3",
   "metadata": {},
   "source": [
    "![](picture/f2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8336fc9-df99-4086-bbfe-422b9cc28306",
   "metadata": {},
   "source": [
    "## Step 6: Project the data \n",
    "Finally, we can transform the data X via the projection matrix  W to obtain a k-dimensional feature subspace.\n",
    "``` python\n",
    "X_proj = X_std.dot(W.T)\n",
    "\n",
    "print(X_proj.shape)\n",
    "(150, 2)\n",
    "```\n",
    "Here, we visualize the transformed data in PCA space of the first two PCs: PC1 and PC2.\n",
    "``` python\n",
    "plt.scatter(X_proj[:, 0], X_proj[:, 1], c = y)\n",
    "plt.xlabel('PC1'); plt.xticks([])\n",
    "plt.ylabel('PC2'); plt.yticks([])\n",
    "plt.title('2 components, captures {} of total variation'.format(cum_explained_variance[1]))\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96350551-c70b-4836-bf8f-de0659afe35f",
   "metadata": {},
   "source": [
    "In continute, we will put all of the above steps into a single class, train it and verify the result with Scikit-learn's PCA model.\n",
    "``` python\n",
    "class MyPCA:\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components   \n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Standardize data \n",
    "        X = X.copy()\n",
    "        self.mean = np.mean(X, axis = 0)\n",
    "        self.scale = np.std(X, axis = 0)\n",
    "        X_std = (X - self.mean) / self.scale\n",
    "        \n",
    "        # Eigendecomposition of covariance matrix       \n",
    "        cov_mat = np.cov(X_std.T)\n",
    "        eig_vals, eig_vecs = np.linalg.eig(cov_mat) \n",
    "        \n",
    "        # Adjusting the eigenvectors that are largest in absolute value to be positive    \n",
    "        max_abs_idx = np.argmax(np.abs(eig_vecs), axis=0)\n",
    "        signs = np.sign(eig_vecs[max_abs_idx, range(eig_vecs.shape[0])])\n",
    "        eig_vecs = eig_vecs*signs[np.newaxis,:]\n",
    "        eig_vecs = eig_vecs.T\n",
    "       \n",
    "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[i,:]) for i in range(len(eig_vals))]\n",
    "        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "        eig_vals_sorted = np.array([x[0] for x in eig_pairs])\n",
    "        eig_vecs_sorted = np.array([x[1] for x in eig_pairs])\n",
    "        \n",
    "        self.components = eig_vecs_sorted[:self.n_components,:]\n",
    "        \n",
    "        # Explained variance ratio\n",
    "        self.explained_variance_ratio = [i/np.sum(eig_vals) for i in eig_vals_sorted[:self.n_components]]\n",
    "        \n",
    "        self.cum_explained_variance = np.cumsum(self.explained_variance_ratio)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        X_std = (X - self.mean) / self.scale\n",
    "        X_proj = X_std.dot(self.components.T)\n",
    "        \n",
    "        return X_proj\n",
    "# ---------------------------------------------------------\n",
    "my_pca = MyPCA(n_components = 2).fit(X)\n",
    "\n",
    "print('Components:\\n', my_pca.components)\n",
    "print('Explained variance ratio from scratch:\\n', my_pca.explained_variance_ratio)\n",
    "print('Cumulative explained variance from scratch:\\n', my_pca.cum_explained_variance)\n",
    "\n",
    "X_proj = my_pca.transform(X)\n",
    "print('Transformed data shape from scratch:', X_proj.shape)\n",
    "Components:\n",
    " [[ 0.52106591 -0.26934744  0.5804131   0.56485654]\n",
    " [ 0.37741762  0.92329566  0.02449161  0.06694199]]\n",
    "Explained variance ratio from scratch:\n",
    " [0.7296244541329989, 0.22850761786701754]\n",
    "Cumulative explained variance from scratch:\n",
    " [0.72962445 0.95813207]\n",
    "Transformed data shape from scratch: (150, 2)\n",
    "```\n",
    "\n",
    "## PCA with Scikit-Learn \n",
    "In this section, we do PCA through Scikit-Learn package as the following.\n",
    "\n",
    "``` python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2).fit(X_std)\n",
    "\n",
    "print('Components:\\n', pca.components_)\n",
    "print('Explained variance ratio:\\n', pca.explained_variance_ratio_)\n",
    "\n",
    "cum_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print('Cumulative explained variance:\\n', cum_explained_variance)\n",
    "\n",
    "X_pca = pca.transform(X_std) # Apply dimensionality reduction to X.\n",
    "print('Transformed data shape:', X_pca.shape)\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c = y)\n",
    "plt.xlabel('PC1'); plt.xticks([])\n",
    "plt.ylabel('PC2'); plt.yticks([])\n",
    "plt.title('2 components, captures {}% of total variation'.format(cum_explained_variance[1].round(4)*100))\n",
    "plt.show()\n",
    "Components:\n",
    " [[ 0.52106591 -0.26934744  0.5804131   0.56485654]\n",
    " [ 0.37741762  0.92329566  0.02449161  0.06694199]]\n",
    "Explained variance ratio:\n",
    " [0.72962445 0.22850762]\n",
    "Cumulative explained variance:\n",
    " [0.72962445 0.95813207]\n",
    "Transformed data shape: (150, 2)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
